{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install specific requirements\n",
    "(once you did it - you can skip it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update torchtext\n",
    "!pip install torchtext -U\n",
    "# Install YouTokenToMe for tokenization\n",
    "!pip install youtokentome"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "unpack .zip from https://translate.yandex.ru/corpus?lang=en in root folder of project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GMu08_zMAY7m"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import youtokentome as yttm\n",
    "\n",
    "vocab_size = 32000\n",
    "PADDING_TOKEN = 0\n",
    "UNK_TOKEN = 1\n",
    "BOS_TOKEN = 2\n",
    "EOS_TOKEN = 3\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# dataset\n",
    "path = 'corpus.en_ru.1m'\n",
    "tokenizer_path = f'{path}_v{vocab_size}.tokenizer'\n",
    "\n",
    "# Model save path.\n",
    "model_save_path = \"model.pth\"\n",
    "\n",
    "\n",
    "def load_files(path):\n",
    "    res = ([], [])\n",
    "    for i, ext in enumerate(['.en', '.ru']):\n",
    "        with open(path + ext, encoding='utf-8') as in_file:\n",
    "            res[i].extend(in_file.readlines())\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vaMjWUZtg38R"
   },
   "source": [
    "## Prepare data\n",
    "(once you did it - you can skip it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "vmfrqBmzyNBY",
    "outputId": "626fb71f-eb23-480e-964c-ba2ae4e5bee3"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 2)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_en, data_ru = load_files(path)\n",
    "\n",
    "raw_data = {'English' : [line for line in data_en], 'Russian': [line for line in data_ru]}\n",
    "\n",
    "df = pd.DataFrame(raw_data, columns=list(raw_data.keys()))\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "loAavSAPym44",
    "outputId": "8e20a85b-d37b-47f4-acc9-73d61add26ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000000, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['en_len'] = df['English'].str.count(' ')\n",
    "df['ru_len'] = df['Russian'].str.count(' ')\n",
    "df.sort_values(['ru_len', 'en_len'], ascending=[True, True], inplace=True)\n",
    "del df['en_len'], df['ru_len']\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "bRQEFSLt3zKx",
    "outputId": "46872603-18f1-4051-bd43-8c0fe02fdd93"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:50000]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yYYvRotrzRq9"
   },
   "outputs": [],
   "source": [
    "# Create train, test, val sets.\n",
    "train, test = train_test_split(df, test_size=0.2)\n",
    "test, val = train_test_split(test, test_size=0.5)\n",
    "train.to_csv('train.csv', index=False)\n",
    "test.to_csv('test.csv', index=False)\n",
    "val.to_csv('val.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s2CY1bO1hMVj"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "IZN-aua3JPTa",
    "outputId": "0287c063-7897-4884-a234-a1638adad763"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size: 32000\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(tokenizer_path):\n",
    "    tokenizer = yttm.BPE(model=tokenizer_path)\n",
    "else:\n",
    "    # Create temp file with data to train tokenizer.\n",
    "    data_en, data_ru = load_files(path)\n",
    "    temp_file_path = 'tokenizer_text.temp'\n",
    "    with open(temp_file_path, 'w', encoding='utf8') as out_file:\n",
    "        out_file.write('\\n'.join(map(str.lower, data_en)))\n",
    "        out_file.write('\\n'.join(map(str.lower, data_ru)))\n",
    "    # Train tokenizer.\n",
    "    tokenizer = yttm.BPE.train(data=temp_file_path, vocab_size=vocab_size, model=tokenizer_path)\n",
    "    os.remove(temp_file_path)\n",
    "print('Vocab size:', tokenizer.vocab_size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b_Ysl0QUn2c3"
   },
   "source": [
    "## Dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "colab_type": "code",
    "id": "PYCTx8o-2C8_",
    "outputId": "fc937e22-73ae-48d9-f3dd-49249f1e25ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 34796 \n",
      "Val: 4374 \n",
      "Test: 4352\n"
     ]
    }
   ],
   "source": [
    "class TextDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    __output_types = { 'id': yttm.OutputType.ID,\n",
    "                       'subword':yttm.OutputType.SUBWORD }\n",
    "\n",
    "    def __init__(self, csv_file, tokenizer, max_len=60, max_len_ratio=1.5):\n",
    "        self.tokenizer = tokenizer\n",
    "        df = pd.read_csv(csv_file)\n",
    "        # Tokenize sentences using tokenizer.\n",
    "        tokenize_lambda = lambda x: self.tokenize(x.lower().strip(), 'subword')\n",
    "        df['eng_enc'] = df.English.apply(tokenize_lambda)\n",
    "        df['rus_enc'] = df.Russian.apply(tokenize_lambda)\n",
    "        # Delete sentences that exceed the max length and max length ratio.\n",
    "        df['en_len'] = df['eng_enc'].str.len()\n",
    "        df['ru_len'] = df['rus_enc'].str.len()\n",
    "        df.query(f'ru_len < {max_len} & en_len < {max_len}', inplace=True)\n",
    "        df.query(f'ru_len < en_len * {max_len_ratio} & ru_len * {max_len_ratio} > en_len', inplace=True)\n",
    "        # Sort the values for less padding in batching.\n",
    "        df.sort_values(['ru_len', 'en_len'], ascending=[False, False], inplace=True)\n",
    "        # TODO: better unpacking\n",
    "        raw_src, raw_tgt = zip(df[['Russian', 'English']].T.values)\n",
    "        src, tgt = zip(df[['rus_enc', 'eng_enc']].T.values)\n",
    "        self.tgt, self.src = tgt[0], src[0]\n",
    "        self.raw_src, self.raw_tgt = raw_src[0], raw_tgt[0]\n",
    "        \n",
    "\n",
    "    def tokenize(self, s, output_type='id'):\n",
    "        \"\"\"Tokenize the sentence.\n",
    "        :param s: the sentence to tokenize\n",
    "        :param output_type: either 'id' or 'subword' for corresponding output\n",
    "        :return: tokenized sentence\"\"\"\n",
    "        return self.tokenizer.encode(s, output_type=self.__output_types[output_type],\n",
    "                                bos=True, eos=True)\n",
    "    def decode(self, tokens):\n",
    "        return self.tokenizer.id_to_subword(tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.src)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "        src = self.src[idx]\n",
    "        src = [self.tokenizer.subword_to_id(token) for token in src]\n",
    "        tgt = self.tgt[idx]\n",
    "        tgt = [self.tokenizer.subword_to_id(token) for token in tgt]\n",
    "        return src, tgt\n",
    "\n",
    "def load_datasets(tokenizer, ext='.csv'):\n",
    "    res = []\n",
    "    for name in  ['train', 'val', 'test']:\n",
    "        dataset_path = name + ext\n",
    "        res.append(TextDataset(dataset_path, tokenizer))\n",
    "    return res\n",
    "\n",
    "train_data, val_data, test_data = load_datasets(tokenizer)\n",
    "print('Train:', len(train_data),\n",
    "      '\\nVal:', len(val_data),\n",
    "      '\\nTest:', len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "awZBdYoP0Cc2"
   },
   "outputs": [],
   "source": [
    "def my_collate(batch):\n",
    "    src, tgt = zip(*batch)\n",
    "    src = [Tensor(s) for s in src]\n",
    "    tgt = [Tensor(t) for t in tgt]\n",
    "    # TODO: Generalize padding value\n",
    "    src = pad_sequence(src, batch_first=True, padding_value=PADDING_TOKEN).long()\n",
    "    tgt = pad_sequence(tgt, batch_first=True, padding_value=PADDING_TOKEN).long()\n",
    "    return src.t(), tgt.t()\n",
    "\n",
    "def make_dataloaders(datasets, batch_size, num_workers=0):\n",
    "    res = []\n",
    "    for dataset in datasets:\n",
    "        res.append(DataLoader(dataset, batch_size=batch_size,\n",
    "                        shuffle=False, num_workers=num_workers, collate_fn=my_collate))\n",
    "    return res\n",
    "\n",
    "(train_iterator,\n",
    " val_iterator,\n",
    " test_iterator) = make_dataloaders([train_data, val_data, test_data],\n",
    "                                   batch_size=8,\n",
    "                                   num_workers=0)\n",
    "\n",
    "data_iterators = {\n",
    "    'train': train_iterator,\n",
    "    'val': val_iterator,\n",
    "    'test': test_iterator,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jYV4b6icn2dB"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_3y5Drcrn2dD"
   },
   "outputs": [],
   "source": [
    "# Source of origin: https://pytorch.org/tutorials/beginner/transformer_tutorial.html\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "        self.d_model = d_model\n",
    "    \n",
    "    def create_pe(self, seq_len):\n",
    "        pe = torch.zeros(seq_len, self.d_model)\n",
    "        position = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, self.d_model, 2).float() * (-math.log(10000.0) / self.d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        return pe\n",
    "\n",
    "    def forward(self, x):\n",
    "        pe = self.create_pe(x.size(0))\n",
    "        x = x + pe.to(x.device)\n",
    "        return self.dropout(x)\n",
    "\n",
    "\n",
    "class TransformerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, ntokens_src, ntokens_tgt, ninp, nhead, dim_feedforward, nlayers, pad_token, dropout=0.1):\n",
    "        super(TransformerModel, self).__init__()\n",
    "        from torch.nn import Transformer\n",
    "        self.model_type = 'Transformer'\n",
    "        self.ninp = ninp\n",
    "        self.pad_token = pad_token\n",
    "        self.masks = {\n",
    "            'src': None,\n",
    "            'tgt': None,\n",
    "            'memory': None,\n",
    "        }\n",
    "        # Token Encoders\n",
    "        self.src_encoder = nn.Embedding(ntokens_src, ninp)\n",
    "        self.tgt_encoder = nn.Embedding(ntokens_tgt, ninp)\n",
    "        # Positional Encoding\n",
    "        self.pos_encoder = PositionalEncoding(ninp, dropout)\n",
    "        # Transformer\n",
    "        self.transformer = Transformer(\n",
    "            d_model=ninp,\n",
    "            nhead=nhead,\n",
    "            num_encoder_layers=nlayers,\n",
    "            num_decoder_layers=nlayers,\n",
    "            dropout=dropout,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "        )\n",
    "        self.out = nn.Linear(ninp, ntokens_tgt)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def _generate_square_subsequent_mask(self, sx, sy=None):\n",
    "        sy = sy or sx\n",
    "        mask = (torch.triu(torch.ones((sx, sy))) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.transformer._reset_parameters()\n",
    "    \n",
    "    def preprocess(self, x, x_type):\n",
    "        # Create masks\n",
    "        padding_mask = (x == self.pad_token).bool().t()\n",
    "        if self.masks[x_type] is None or self.masks[x_type].size(0) != len(x):\n",
    "            self.masks[x_type] = self._generate_square_subsequent_mask(len(x), len(x)).to(x.device)\n",
    "        \n",
    "        x_enc = self.src_encoder(x) if x_type == 'src' else self.tgt_encoder(x)\n",
    "        x_enc *= math.sqrt(self.ninp) # TODO: * or / or remove?\n",
    "        x_enc = self.pos_encoder(x_enc)\n",
    "        \n",
    "        return x_enc, self.masks[x_type], padding_mask\n",
    "        \n",
    "    def forward(self, src, tgt):\n",
    "\n",
    "        if (    self.masks['memory'] is None or\n",
    "                self.masks['src'].size(0) != len(src) or\n",
    "                self.masks['tgt'].size(0) != len(tgt)):\n",
    "            self.masks['memory'] = self._generate_square_subsequent_mask(len(src), len(tgt)).to(src.device)\n",
    "        \n",
    "        src_enc, _, src_key_padding_mask = self.preprocess(src, 'src')\n",
    "        tgt_enc, _, tgt_key_padding_mask = self.preprocess(tgt, 'tgt')\n",
    "        memory_key_padding_mask = src_key_padding_mask.clone().detach()\n",
    "        \n",
    "        output = self.transformer(src_enc, tgt_enc,\n",
    "                                  src_mask=self.masks['src'],\n",
    "                                  tgt_mask=self.masks['tgt'],\n",
    "                                  memory_mask=self.masks['memory'],\n",
    "                                  src_key_padding_mask=src_key_padding_mask,\n",
    "                                  tgt_key_padding_mask=tgt_key_padding_mask,\n",
    "                                  memory_key_padding_mask=memory_key_padding_mask,\n",
    "                                  )\n",
    "        output = self.out(output)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9apm7uen2dv"
   },
   "outputs": [],
   "source": [
    "def run_model(model, criterion, optimizer, data_iterator, is_train_phase, n_words=1, desc=''):\n",
    "    if is_train_phase:\n",
    "        model.train()\n",
    "    else:\n",
    "        model.eval()\n",
    "    total_loss = 0.0\n",
    "    pbar = tqdm(total=len(data_iterator), desc=desc)\n",
    "    for i, (src, tgt) in enumerate(data_iterator):\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        \n",
    "        tgt_losses = 0.0\n",
    "        for j in range(max(1, len(tgt) - n_words), len(tgt)):\n",
    "            optimizer.zero_grad()\n",
    "            tgt_in = tgt[:j, :]\n",
    "            tgt_out = tgt[1:j+1, :]\n",
    "            \n",
    "            with torch.set_grad_enabled(is_train_phase):\n",
    "                output = model(src, tgt_in).transpose(1, 2)\n",
    "                loss = criterion(output, tgt_out)\n",
    "\n",
    "                if is_train_phase:\n",
    "                    loss.backward()\n",
    "                    # Clip gradient to deal with gradient explosion\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "                    optimizer.step()\n",
    "            tgt_losses += loss.item()\n",
    "        total_loss += tgt_losses / j\n",
    "        pbar.update(1)\n",
    "        pbar.set_description(desc + f'- loss: {total_loss / (i+1):7.4}')\n",
    "    return total_loss / (i+1)\n",
    "\n",
    "def train_model(model, n_epochs, data_iterators,\n",
    "                criterion, optimizer, n_words=1, scheduler=None, model_save_path=None):\n",
    "    stats = {'train':{'loss':[]},\n",
    "             'val':{'loss':[]}}\n",
    "    best_loss = None\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        lr = optimizer.state_dict()['param_groups'][0]['lr']\n",
    "        print(f'------------ Epoch {epoch}; lr: {lr:.5f} ------------')\n",
    "        for phase in ['train', 'val']:\n",
    "            desc = f'{phase.title()} Epoch #{epoch} '\n",
    "            epoch_loss = run_model(model, criterion, optimizer,\n",
    "                                   data_iterators[phase], phase == 'train',\n",
    "                                   n_words, desc)\n",
    "            stats[phase]['loss'].append(epoch_loss)\n",
    "            print_hist = lambda l: ' -> '.join(map(lambda x:f\"{x:.4}\", l[-2:]))\n",
    "            tqdm.write(f'{phase.title()} Loss: ' + print_hist(stats[phase]['loss']))\n",
    "        if best_loss == None or stats['val']['loss'][-1] < best_loss:\n",
    "            best_loss = stats['val']['loss'][-1]\n",
    "            print('Smallest val loss')\n",
    "            if model_save_path:\n",
    "                print('Saving model...')\n",
    "                torch.save(model, model_save_path)\n",
    "        translate(model, 'Машинное обучение это здорово!', verbose=True)\n",
    "        scheduler.step()\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgTkNH_OeBHm"
   },
   "outputs": [],
   "source": [
    "ntokens_src = tokenizer.vocab_size() # the size of vocabulary\n",
    "ntokens_tgt = tokenizer.vocab_size() # the size of vocabulary\n",
    "emsize = 512 # embedding dimension\n",
    "nhid = 1024 # the dimension of the feedforward network model in nn.TransformerEncoder\n",
    "nlayers = 5 # the number of nn.TransformerEncoderLayer in nn.TransformerEncoder\n",
    "nhead = 8 # the number of heads in the multiheadattention models\n",
    "dropout = 0.1 # the dropout value\n",
    "model = TransformerModel(ntokens_src, ntokens_tgt, emsize, nhead, nhid, nlayers, PADDING_TOKEN, dropout).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g_vrH5KIip1D"
   },
   "outputs": [],
   "source": [
    "# Ignore padding index during the loss computation.\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PADDING_TOKEN, reduction='mean')\n",
    "lr = 0.8\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.97)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 66,
     "referenced_widgets": [
      "246ccc8828af4d3bb2277ef42f965f30",
      "587bcb7039be4be5bbeeb4b3d8a6229f",
      "8737f0a9fec64bde86a885d21fa3e9d4",
      "60b705930fe54b29aee27c12fec9b07d",
      "85942e0eb81140a984822a8580063404",
      "6c2e3fba27704cd09c87d55c0dd12813",
      "6e4ee98f0e0243a2adda5aa2f2c1168d",
      "7ebb5b82d39c4152aafc2efdeb6ff38a",
      "f3603c12cff64bd28dea230b400c816d",
      "324c9379030044eeb1aa8f91bed66782"
     ]
    },
    "colab_type": "code",
    "id": "vXr1J0fQ_TP9",
    "outputId": "0a24d00c-04b7-4455-db2a-74bf1eb6fa58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Epoch 0; lr: 0.77600 ------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "246ccc8828af4d3bb2277ef42f965f30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train Epoch #0 ', max=4002.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "\r",
      "\r",
      "Train Loss: 0.466\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3603c12cff64bd28dea230b400c816d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Val Epoch #0 ', max=502.0, style=ProgressStyle(descriptio…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "\r",
      "\r",
      "\r",
      "\r",
      "Val Loss: 0.6473\n",
      "Smallest val loss\n",
      "Saving model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type TransformerModel. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n",
      "/usr/local/lib/python3.6/dist-packages/torch/serialization.py:360: UserWarning: Couldn't retrieve source code for container of type PositionalEncoding. It won't be checked for correctness upon loading.\n",
      "  \"type \" + obj.__name__ + \". It won't be checked \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved successfully\n",
      "------------ Translation ------------\n",
      "Input: Машинное обучение это здорово!\n",
      "Output weights:\n",
      "  0 {'▁not': 0.009476497769355774, '▁it': 0.006838961038738489, '▁other': 0.006023898255079985}\n",
      "  1 {'▁so': 0.013619394041597843, '▁not.': 0.013337705284357071, '▁very': 0.011635693721473217}\n",
      "  2 {'▁not.': 0.04119184985756874, '▁well.': 0.028293495997786522, '▁possible.': 0.022640258073806763}\n",
      "translation: <BOS> not so not.<EOS>\n",
      "------------ Epoch 1; lr: 0.75272 ------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324c9379030044eeb1aa8f91bed66782",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Train Epoch #1 ', max=4002.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "n_epochs = 2\n",
    "n_words = 1\n",
    "stats = train_model(model, n_epochs, data_iterators,\n",
    "                    criterion, optimizer, n_words, scheduler, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NtY1RCJnLQA_"
   },
   "outputs": [],
   "source": [
    "def subword_to_str(tokens):\n",
    "    return ''.join(tokens).replace('▁', ' ')\n",
    "\n",
    "def tokens_to_str(tokens):\n",
    "    return subword_to_str([tokenizer.id_to_subword(ix) for ix in tokens])\n",
    "\n",
    "def translate(model, text, max_len=80, custom_string=False, verbose=False):\n",
    "    model.eval()\n",
    "    \n",
    "    if verbose:\n",
    "        print('------------ Translation ------------')\n",
    "        print('Input:', text)\n",
    "    # Prepare text\n",
    "    src = tokenizer.encode(text, output_type=yttm.OutputType.ID,\n",
    "                           bos=True, eos=True)\n",
    "    src = Tensor(src).long().to(device)\n",
    "    # Run encoder\n",
    "    src_enc, src_mask, _ = model.preprocess(src, 'src')\n",
    "    e_outputs = model.transformer.encoder(src_enc, \n",
    "                                          src_mask,\n",
    "                                          )\n",
    "    \n",
    "    # Prepare tensor for answers\n",
    "    outputs = torch.zeros(max_len).type_as(src.data)\n",
    "    # Set the first token as '<sos>'\n",
    "    outputs[0] = torch.LongTensor([BOS_TOKEN])\n",
    "    vals = []\n",
    "    for i in range(1, max_len):\n",
    "        outputs_enc, tgt_mask, _ = model.preprocess(outputs[:i].unsqueeze(1), 'tgt')\n",
    "        d_out = model.transformer.decoder(outputs_enc, e_outputs,\n",
    "                                          tgt_mask=tgt_mask,\n",
    "                                          )\n",
    "        out = model.out(d_out)\n",
    "        out = F.softmax(out, dim=-1)\n",
    "        val, ix = out.data.topk(3, dim=-1)\n",
    "        outputs[i] = ix[-1][0][0]\n",
    "        if outputs[i] == EOS_TOKEN:\n",
    "            break\n",
    "    result = tokens_to_str(outputs[:i+1])\n",
    "    if verbose:\n",
    "        print('Output weights:')\n",
    "        for j in range(min(3, i)):\n",
    "            print(f'  {j}', {tokenizer.id_to_subword(k):v.item()\n",
    "                             for k, v in zip(ix[j][0], val[j][0])})\n",
    "        print('translation:', result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 175
    },
    "colab_type": "code",
    "id": "enitOd_CNx8k",
    "outputId": "0c28e989-7121-447b-dca7-73559fe8ff5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Translation ------------\n",
      "Input: Машинное обучение это здорово!\n",
      "Output weights:\n",
      "  0 {'▁the': 0.13046355545520782, '▁in': 0.04928888380527496, '▁it': 0.045001816004514694}\n",
      "  1 {'▁the': 0.08483277261257172, '▁in': 0.034705374389886856, '▁i': 0.03179672732949257}\n",
      "  2 {'▁the': 0.06560049206018448, '▁in': 0.02773398533463478, '▁i': 0.027686230838298798}\n",
      "translation: <BOS> the the the the the the the the the the the the the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<BOS> the the the the the the the the the the the the the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the first the'"
      ]
     },
     "execution_count": 20,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate(model, 'Машинное обучение это здорово!', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HefsiVVvbqtV"
   },
   "outputs": [],
   "source": [
    "def translate_beam(model, text, max_len=10, beam_capacity=3, verbose=False):\n",
    "    \"\"\"\n",
    "    Algorithm: https://www.youtube.com/watch?v=RLWuzLLSIgw\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    if verbose:\n",
    "        print('------------ Translation ------------')\n",
    "        print('Input:', text)\n",
    "    # Prepare text\n",
    "    src = tokenizer.encode(text, output_type=yttm.OutputType.ID,\n",
    "                           bos=True, eos=True)\n",
    "    src = Tensor(src).long().to(device)\n",
    "    # Run encoder\n",
    "    src_enc, src_mask, _ = model.preprocess(src, 'src')\n",
    "    e_outputs = model.transformer.encoder(src_enc, \n",
    "                                          src_mask,\n",
    "                                          )\n",
    "\n",
    "    # Prepare tensor for answers\n",
    "    basic_vec = torch.zeros(max_len).type_as(src.data)\n",
    "    basic_vec[0] = torch.LongTensor([BOS_TOKEN])\n",
    "\n",
    "    beam_pool = [(basic_vec, 1.0)]\n",
    "\n",
    "    def beam_filter(pool, top_k=beam_capacity):\n",
    "        return sorted(pool, key=lambda x: x[1], reverse=True)[:top_k]\n",
    "\n",
    "    for i in range(1, max_len):\n",
    "        if verbose:\n",
    "            print(\"Beam epoch: \", i)\n",
    "        new_pool = []\n",
    "        # For each candidate path:\n",
    "        for beam, old_prob in beam_pool:\n",
    "            outputs_enc, tgt_mask, _ = model.preprocess(beam[:i].unsqueeze(1), 'tgt')\n",
    "            d_out = model.transformer.decoder(outputs_enc, e_outputs,\n",
    "                                              tgt_mask=tgt_mask,\n",
    "                                              )\n",
    "            out = model.out(d_out)\n",
    "            out = F.softmax(out, dim=-1)\n",
    "            probs, ixs = out[-1, :].topk(beam_capacity)\n",
    "            for prob, token_id in zip(probs.squeeze(), ixs.squeeze()):\n",
    "                tmp_beam = beam.clone()\n",
    "                tmp_beam[i] = token_id.item()\n",
    "                new_pool.append((tmp_beam, prob * old_prob))\n",
    "        beam_pool = beam_filter(new_pool)\n",
    "        if verbose:\n",
    "            for beam, old_prob in beam_pool:\n",
    "                print(\"Candidate '{}' with prob: {:.7f}\".format(\n",
    "                    tokens_to_str(beam[1:i + 1]), prob * old_prob\n",
    "                ))\n",
    "        # Stop if EOS_TOKEN\n",
    "        if beam_pool[0][0][i] == EOS_TOKEN:\n",
    "            break\n",
    "    the_best = beam_filter(beam_pool, 1)[0][0]\n",
    "    result = tokens_to_str(the_best[:i+1])\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 693
    },
    "colab_type": "code",
    "id": "Nfq8TVJmfHar",
    "outputId": "cacbc7f4-abca-437d-8309-1e0995647a6c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Translation ------------\n",
      "Input: Машинное обучение это здорово!\n",
      "Beam epoch:  1\n",
      "Candidate 'но-демократи' with prob: 0.0000000\n",
      "Candidate ' fruit,' with prob: 0.0000000\n",
      "Candidate 'man,' with prob: 0.0000000\n",
      "Beam epoch:  2\n",
      "Candidate 'но-демократино-демократи' with prob: 0.0000000\n",
      "Candidate 'но-демократи той' with prob: 0.0000000\n",
      "Candidate 'но-демократи publications,' with prob: 0.0000000\n",
      "Beam epoch:  3\n",
      "Candidate 'но-демократино-демократи той' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократино-демократи' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократи publications,' with prob: 0.0000000\n",
      "Beam epoch:  4\n",
      "Candidate 'но-демократино-демократино-демократи той' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократи той publications,' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократи publications, единым' with prob: 0.0000000\n",
      "Beam epoch:  5\n",
      "Candidate 'но-демократино-демократино-демократи той publications,' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократи той publications, единым' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократи publications, единымно-демократи' with prob: 0.0000000\n",
      "Beam epoch:  6\n",
      "Candidate 'но-демократино-демократи publications, единымно-демократи той' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократино-демократи той publications, единым' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократи той publications, единым true,' with prob: 0.0000000\n",
      "Beam epoch:  7\n",
      "Candidate 'но-демократино-демократино-демократи той publications, единым true,' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократино-демократи той publications, единым publications,' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократино-демократи той publications, единымно-демократи' with prob: 0.0000000\n",
      "Beam epoch:  8\n",
      "Candidate 'но-демократино-демократино-демократи той publications, единымно-демократи той' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократино-демократи той publications, единым publications, единым' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократино-демократи той publications, единым publications, опере' with prob: 0.0000000\n",
      "Beam epoch:  9\n",
      "Candidate 'но-демократино-демократино-демократи той publications, единымно-демократи той publications,' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократино-демократи той publications, единым publications, единым true,' with prob: 0.0000000\n",
      "Candidate 'но-демократино-демократино-демократи той publications, единымно-демократи той той' with prob: 0.0000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<BOS>но-демократино-демократино-демократи той publications, единымно-демократи той publications,'"
      ]
     },
     "execution_count": 31,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_beam(model, 'Машинное обучение это здорово!', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "colab_type": "code",
    "id": "iDw4bGozu6xf",
    "outputId": "d30dbe4b-71a5-4bf4-ebfd-6ddd0a5dc11d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------ Translation ------------\n",
      "Input: Затем по нему пробежала вереница символов и стандартных заголовков: \"Загрузка черепной коробки\".\n",
      "\n",
      "Beam epoch:  1\n",
      "Candidate 'рующими' with prob: 0.0000000\n",
      "Candidate ' установленный' with prob: 0.0000000\n",
      "Candidate ' глаго' with prob: 0.0000000\n",
      "Beam epoch:  2\n",
      "Candidate 'рующими увяз' with prob: 0.0000000\n",
      "Candidate 'рующими установленный' with prob: 0.0000000\n",
      "Candidate 'рующими poet' with prob: 0.0000000\n",
      "Beam epoch:  3\n",
      "Candidate 'рующими увяз partial' with prob: 0.0000000\n",
      "Candidate 'рующими увязно-демократи' with prob: 0.0000000\n",
      "Candidate 'рующими увязрующими' with prob: 0.0000000\n",
      "Beam epoch:  4\n",
      "Candidate 'рующими увяз partial увяз' with prob: 0.0000000\n",
      "Candidate 'рующими увяз partial get' with prob: 0.0000000\n",
      "Candidate 'рующими увязрующими увяз' with prob: 0.0000000\n",
      "Beam epoch:  5\n",
      "Candidate 'рующими увяз partial get увяз' with prob: 0.0000000\n",
      "Candidate 'рующими увязрующими увяз partial' with prob: 0.0000000\n",
      "Candidate 'рующими увяз partial увяз partial' with prob: 0.0000000\n",
      "Beam epoch:  6\n",
      "Candidate 'рующими увяз partial get увяз увяз' with prob: 0.0000000\n",
      "Candidate 'рующими увяз partial get увяз partial' with prob: 0.0000000\n",
      "Candidate 'рующими увяз partial get увяз единым' with prob: 0.0000000\n",
      "Beam epoch:  7\n",
      "Candidate 'рующими увяз partial get увяз увяз увяз' with prob: 0.0000000\n",
      "Candidate 'рующими увяз partial get увяз partial get' with prob: 0.0000000\n",
      "Candidate 'рующими увяз partial get увяз partial увяз' with prob: 0.0000000\n",
      "Beam epoch:  8\n",
      "Candidate 'рующими увяз partial get увяз partial get увяз' with prob: 0.0000000\n",
      "Candidate 'рующими увяз partial get увяз увяз увяз увяз' with prob: 0.0000000\n",
      "Candidate 'рующими увяз partial get увяз partial увяз увяз' with prob: 0.0000000\n",
      "Beam epoch:  9\n",
      "Candidate 'рующими увяз partial get увяз partial get увяз увяз' with prob: 0.0000000\n",
      "Candidate 'рующими увяз partial get увяз увяз увяз увяз увяз' with prob: 0.0000000\n",
      "Candidate 'рующими увяз partial get увяз partial get увяз partial' with prob: 0.0000000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<BOS>рующими увяз partial get увяз partial get увяз увяз'"
      ]
     },
     "execution_count": 32,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "translate_beam(model, train_data.raw_src[index], verbose=True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "torchtext_translation_tutorial (1).ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "ml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "246ccc8828af4d3bb2277ef42f965f30": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8737f0a9fec64bde86a885d21fa3e9d4",
       "IPY_MODEL_60b705930fe54b29aee27c12fec9b07d"
      ],
      "layout": "IPY_MODEL_587bcb7039be4be5bbeeb4b3d8a6229f"
     }
    },
    "587bcb7039be4be5bbeeb4b3d8a6229f": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "60b705930fe54b29aee27c12fec9b07d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7ebb5b82d39c4152aafc2efdeb6ff38a",
      "placeholder": "​",
      "style": "IPY_MODEL_6e4ee98f0e0243a2adda5aa2f2c1168d",
      "value": " 4002/4002 [44:36&lt;00:00,  3.34it/s]"
     }
    },
    "6c2e3fba27704cd09c87d55c0dd12813": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6e4ee98f0e0243a2adda5aa2f2c1168d": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7ebb5b82d39c4152aafc2efdeb6ff38a": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "85942e0eb81140a984822a8580063404": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "8737f0a9fec64bde86a885d21fa3e9d4": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "Train Epoch #0 - loss:   0.466: 100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6c2e3fba27704cd09c87d55c0dd12813",
      "max": 4002,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_85942e0eb81140a984822a8580063404",
      "value": 4002
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
